Timers are useful for measuring short-duration latencies and the frequency of such events. All implementations of `Timer` report at least the total time and count of events as separate time series.

As an example, consider a graph showing request latency to a typical web server. The server can be expected to respond to many requests quickly, so the timer will be getting updated many times per second.

ifeval::["{system}" == "atlas"]
:baseunit: Atlas expects nanoseconds as its base unit of time
endif::[]

ifeval::["{system}" == "datadog"]
:baseunit: Datadog expects nanoseconds as its base unit of time
endif::[]

ifeval::["{system}" == "prometheus"]
:baseunit: Prometheus expects seconds as its base unit of time
endif::[]


The appropriate base unit for timers varies by metrics backend for good reason. {baseunit}. Micrometer is decidedly un-opinionated about this, but because of the potential for confusion, requires a `TimeUnit` when interacting with `Timers`. Micrometer is aware of the preferences of each implementation and stores your timing in the appropriate base unit based on the implementation.

[source,java]
----
public interface Timer extends Meter {
    ...
    void record(long amount, TimeUnit unit);
    double totalTime(TimeUnit unit);
}
----

ifeval::["{system}" == "atlas"]
While reading directly from a `Timer` returns a `double`, the underlying value
stored in https://github.com/netflix/spectator[Spectator] as a nanosecond-precise `long`. What precision is lost by
converting to a `double` in the `Timer` interface will not affect a system like
Atlas, because it will be configured to read measurements from the underlying
Spectator `Timer` that `spring-metrics` is hiding from you.

The Spectator Atlas `Timer` produces four time series, each with a different `statistic` tag:

1. `count` - Rate of calls per second.
2. `totalTime` - Rate of total time per second.
3. `totalOfSquares` - Rate of total time squared per second (useful for standard deviation).
4. `max` - The maximum amount recorded.

Therefore, a throughput (requests/second) line can be achieved with this query:

```http
name,timer,:eq,statistic,count,:eq,:and
```

Notice that `statistic` is just a dimension that can be drilled down and selected like any other.

Furthermore, `totalTime/count` represents average latency, and can be selected with a short-hand `:dist-avg` query, which selects the `totalTime` and `count` time series and performs the division for us:

```http
name,timer,:eq,:dist-avg
```

In the example, you see these two lines plotted on a single dual-axis graph.

.Timer over a simulated service.
image::img/atlas-timer.png[Atlas-rendered timer]

endif::[]

ifeval::["{system}" == "datadog"]

The Datadog `Timer` produces four time series, each with a different `statistic` tag:

1. `count` - Rate of calls per second.
2. `totaltime` - Rate of total time per second.
3. `totalofsquares` - Rate of total time squared per second (useful for standard deviation).
4. `max` - The maximum amount recorded.

.Timer over a simulated service.
image::img/datadog-timer.png[Datadog-rendered timer,float="right"]

To generate a graph of average latency, we divide the time series by the count, as follows: `totaltime/count`. The `count` series alone gives you a measure of throughput in requests per second.

*Average Latency Query*
[source,json]
----
{
  "requests": [
    {
      "q": "avg:timer{statistic:totaltime} / avg:timer{statistic:count}",
      "type": "line",
      "conditional_formats": [],
      "aggregator": "avg"
    }
  ],
  "viz": "timeseries",
  "autoscale": true
}
----
endif::[]

ifeval::["{system}" == "prometheus"]
.Timer over a simulated service.
image::img/prometheus-timer.png[Grafana-rendered Prometheus timer,float="right"]

The Prometheus `Timer` produces two counter time series with different names:

1. `${name}_count` - Total number of all calls.
2. `${name}_sum` - Total time of all calls.

Representing a counter without rate normalization over some time window is rarely useful, as the representation is a function of both the rapidity with which the counter is incremented and the longevity of the service. It is generally most useful to rate normalize these time series to reason about them. Since Prometheus keeps track of discrete events across all time, it has the advantage of allowing for the selection of an arbitrary time window across which to normalize at query time (for example, `rate(timer_count[10s])` provides a notion of requests per second over 10 second windows).

*Prometheus Queries*

1. Average latency: `rate(timer_sum[10s])/rate(timer_count[10s])`
2. Throughput (requests per second): `rate(timer_count[10s])`
endif::[]
